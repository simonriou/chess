# Chess Evaluation Neural Network â€“ Training Schedule

## ğŸ§  Phase 0: Base Model Summary

- **Input**: `19Ã—8Ã—8` tensor (12 pieces + 7 auxiliary channels)
- **Model**: 10 residual blocks, Conv2D (128 filters, 3Ã—3)
- **Output**: `Dense(1, activation='tanh')` â†’ scalar eval
- **Loss**: `MeanSquaredError`
- **Eval Metric**: `MAE` or domain-specific (e.g. sign accuracy)

---

## ğŸš€ Phase 1: Initial Training

### âš™ï¸ Training Hyperparameters

| Parameter             | Value                            |
|----------------------|----------------------------------|
| Batch size           | 256                              |
| Learning rate        | 1e-3 (Adam)                      |
| Scheduler            | Cosine decay or ReduceLROnPlateau |
| Optimizer            | Adam                             |
| Loss                 | MSE                              |
| Epochs               | 50                               |
| Early stopping       | Patience = 5                     |
| Shuffling            | Yes (reshuffle each epoch)       |
| Caching / Prefetch   | Yes (for performance)            |
| Data split           | 95% train / 5% val               |

### ğŸ§ª Monitoring Metrics

- Train & val loss (MSE, MAE)
- Eval distribution (histogram of predictions)
- Eval direction accuracy (sign match)

---

## ğŸ§­ Phase 2: Evaluation After First Run

| Symptom                                 | Action                                                    |
|----------------------------------------|-----------------------------------------------------------|
| âš ï¸ Overfitting                         | Add dropout (0.2â€“0.4), reduce model width                 |
| ğŸ“‰ Underfitting                        | Add more residual blocks or filters                      |
| ğŸ” Early learning plateau              | Use warmup + cosine decay, reduce LR                     |
| ğŸ”„ Predictions saturate at Â±1.0        | Check label normalization, add residual depth            |
| ğŸš« MAE plateaus at ~0.3â€“0.4            | Use Huber loss or scaled log-cosh                        |

---

## ğŸ›  Phase 3: Improvements if Needed

### ğŸ”¹ Option 1: Expand Model Capacity
- Add more residual blocks (15â€“20)
- Increase Conv2D filters (128 â†’ 256)
- Add Squeeze-and-Excitation layers

### ğŸ”¹ Option 2: Improve Label Strategy
- Apply soft label smoothing near Â±1.0
- Cap centipawn evals more conservatively (e.g. Â±10,000)

### ğŸ”¹ Option 3: Multitask Learning
- Add policy head (distribution over legal moves)
- Jointly train value + policy heads

### ğŸ”¹ Option 4: Curriculum Learning
- Train first on â€œsimpleâ€ positions (low cp eval)
- Gradually introduce harder, high-magnitude examples

---

## ğŸ“Š Visualization & Logging

- TensorBoard:
  - Loss curves
  - Learning rate
  - Output histograms
- Save samples: prediction vs ground truth, especially near 0 and Â±1